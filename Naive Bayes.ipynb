{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian classification and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier (discrete)\n",
    "\n",
    "Consider a supervised binary classification problem for discrete vectors $x=(x^i,i=1,...,n)$ given a training set\n",
    "$$\n",
    "\\{(y_j, x_j), j=1..N\\},\n",
    "$$\n",
    "where $x_j=(x^i_j,i=1,...,n)$ are given observed vectors with discrete components and $y_j$ are the corresponding binary (or more generally - discrete) classifiction labels. \n",
    "\n",
    "In order to predict if a given $x=x^*$ should be classified as $0$ or $1$ (or more generally as one of the finite set of labels) we compare two probabilities\n",
    "$P(y=1|x=x^*)$ vs $P(y=0|x=x^*)$ defined as a posteriors using Bayes theorem\n",
    "$$\n",
    "P(y=b|x)=\\frac{P(x|y=b)P(y=b)}{P(x)}\n",
    "$$\n",
    "based on the priors $P(y=b)$ and conditional probabilities \n",
    "$$\n",
    "P(x=x^*|y=b)=\\prod\\limits_{i=1}^n P(x^i={x^*}^i |y=b)\n",
    "$$\n",
    "in the \"naive\" assumption of independence for the components $x^i$. The above probabilities $P(y=b)$ and $P(x^i={x^*}^i |y=b)$ can be derived directly from the training sample as\n",
    "$$\n",
    "P(y=b)=\\frac{|\\{j:y_j=b\\}|}{N},\n",
    "$$\n",
    "$$\n",
    "P(x^i={x^*}^i |y=b)=\\frac{|\\{j:x^i_j={x^*}^i, y_j=b\\}|}{|\\{j:y_j=b\\}|}.\n",
    "$$\n",
    "\n",
    "As the denominator $P(x)$ does not depend on $b$ it is enough to compare the numerators, one can see \n",
    "$$\n",
    "P(y=b|x=x^*)\\sim P(y=b)\\prod\\limits_{i=1}^n P(x^i={x^*}^i |y=b),\n",
    "$$\n",
    "letting the clasification level to be defined as\n",
    "$$\n",
    "y^*={\\rm argmax}_{b}P(y=b)\\prod\\limits_{i=1}^n P(x^i={x^*}^i |y=b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. \"Toy\" example of spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "#import shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Win</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cruise  Lottery  Win  spam\n",
       "0       1        1    1     1\n",
       "1       1        0    1     1\n",
       "2       0        1    0     1\n",
       "3       0        0    1     0\n",
       "4       1        0    0     0\n",
       "5       0        1    0     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consider some data about content of several e-mails classified as Spam and non-spam (Ham)\n",
    "train=pd.DataFrame({'spam':[1, 1, 1, 0, 0, 0],'Lottery':[1, 0, 1, 0, 0, 1],'Cruise':[1, 1, 0, 0, 1, 0],'Win':[1, 1, 0, 1, 0, 0]})\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cruise</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cruise  Lottery  Win\n",
       "0       1        1    0\n",
       "1       0        0    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now if we receive emails of the following content - are those spam or not?\n",
    "test=pd.DataFrame({'Lottery':[1, 0],'Cruise':[1, 0],'Win':[0, 0]})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First estimate the prior:\n",
    "$$\n",
    "P(spam)=P(ham)=3/6=1/2.\n",
    "$$\n",
    "Now estimate the conditional probabilities of having each word (probability of not having it will then be one minus the above probability) if the e-mail is spam or not:\n",
    "$$\n",
    "P(Cruise|spam)=2/3\n",
    "$$\n",
    "$$\n",
    "P(Lottery|spam)=2/3\n",
    "$$\n",
    "$$\n",
    "P(Win|spam)=2/3\n",
    "$$\n",
    "$$\n",
    "P(Cruise|ham)=1/3\n",
    "$$\n",
    "$$\n",
    "P(Lottery|ham)=1/3\n",
    "$$\n",
    "$$\n",
    "P(Win|ham)=1/3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first of those two emails containing words \"Cruise\" and \"Lottery\" (denote it \"{Cruise,Lottery}\"). As\n",
    "$$\n",
    "P(spam|\\{Cruise,Lottery\\})\\sim P(Cruise|spam)P(Lottery|spam)P(!Win|spam)P(spam)=2/3*2/3*(1-2/3)*1/2=2/27\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(ham|\\{Cruise,Lottery\\})\\sim P(Cruise|ham)P(Lottery|ham)P(!Win|ham)P(ham)=1/3*1/3*(1-1/3)*1/2=1/27\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "P(spam|\\{Cruise,Lottery\\})>P(ham|\\{Cruise,Lottery\\})\\Rightarrow \\{Cruise,Lottery\\}\\to spam.\n",
    "$$\n",
    "So the classifier will mark it as \"spam\".\n",
    "\n",
    "For the second e-mail containing none of the words above:\n",
    "$$\n",
    "P(spam|none)\\sim P(!Cruise|spam)P(!Lottery|spam)P(!Win|spam)P(spam)=1/3*1/3*1/3*1/2=1/54\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(ham|none)\\sim P(!Cruise|ham)P(!Lottery|ham)P(!Win|ham)P(ham)=2/3*2/3*2/3*1/2=4/27\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "P(spam|\\{Win\\})<P(ham|\\{Win\\})\\Rightarrow \\{Win\\}\\to ham.\n",
    "$$\n",
    "So the classifier will mark it as \"ham\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Taxi trips within and outside Manhattan\n",
    "\n",
    "Below we apply Naive Bayes Classifier to classying taxi trips. Given a certain sample of workday daytime taxi trips with the information of how long and fast the trip was as well and how many passengers were on it as how big was the tip (all but the passenger count encoded as discrete categorical variables starting from 1 (smallest) and up to 4-6 (highest)) we need to train a classifier of whether the trip happened inside Manhattan (2) or not (1). Trips from  the sample are assigned such labels to serve for the supervised learning purposes as well as for testing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manhattan</th>\n",
       "      <th>tip</th>\n",
       "      <th>dist</th>\n",
       "      <th>speed</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26066</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26100</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26354</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26722</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26793</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       manhattan  tip  dist  speed  pass\n",
       "26066          0    1     1      1     1\n",
       "26100          0    3     5      6     1\n",
       "26354          0    1     6      4     2\n",
       "26722          0    4     1      1     1\n",
       "26793          1    3     3      3     2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load the sample and split it randomly into the training (75%) and test (25%) set. \n",
    "data = pd.read_csv(\"data/NYC_taxi_sample.csv\",index_col=0)\n",
    "data.manhattan=data.manhattan-1\n",
    "random.seed(2015)\n",
    "ind=stats.bernoulli.rvs(p = 0.75, size = len(data.index))\n",
    "train=data[ind==1]\n",
    "test=data[ind==0]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5329, 4061, 1268]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how big is the entire sample as well as the training/test sets\n",
    "[len(data.index),len(train.index),len(test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the estimator for the training sample statistics $P(x=x^*|y=b)$ and $P(y=b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNaiveBayesDiscrete(trainData):\n",
    "  #training discrete Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  m=max([trainData[j][i] for j in trainData.columns[1:] for i in trainData.index]) #maximal number of classes in each feature of a training set\n",
    "  #create output data structure for the probabilities - same column labels, rows correspond to values of x and there are two arrays like that for different b\n",
    "  dp=[pd.DataFrame(columns=trainData.columns, index=range(1,m+1)), pd.DataFrame(columns=trainData.columns, index=range(1,m+1))]\n",
    "  #split the training data between two labels\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  #estimate P(y=b)  \n",
    "  dp[0][trainData.columns[0]][1]=1.0*ind1.sum()/len(trainData.index)\n",
    "  dp[1][trainData.columns[0]][1]=1.0*ind2.sum()/len(trainData.index)\n",
    "  #estimate conditional probabilities P(x|y=b)\n",
    "  for j in trainData.columns[1:]:\n",
    "    for i in range(1,m+1):\n",
    "        dp[0].loc[i,j]=1.0*(trainData[j][ind1]==i).sum()/ind1.sum();\n",
    "        dp[1].loc[i,j]=1.0*(trainData[j][ind2]==i).sum()/ind2.sum();\n",
    "  return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNaiveBayesDiscrete(classData,dp):\n",
    "  #classifying using trained discrete Naive Bayes Classifier\n",
    "  Y=classData[classData.columns[0]]*0 #initialize the empty array \n",
    "  for i in classData.index: #for al records to classify\n",
    "    #start with the priors\n",
    "    P1=dp[0][classData.columns[0]][1]; \n",
    "    P2=dp[1][classData.columns[0]][1];\n",
    "    #and multiply them by the corresponding conditional probabilities P(x_i|y=b)\n",
    "    for j in classData.columns[1:]:\n",
    "      P1=P1*dp[0][j][classData[j][i]]\n",
    "      P2=P2*dp[1][j][classData[j][i]]\n",
    "    Y[i]=int(P2>P1) #finally for each record decide which P(y|x) is higher and choose the label\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now train the classifier based on the training sample\n",
    "dp=trainNaiveBayesDiscrete(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   manhattan         tip       dist      speed        pass\n",
       " 1  0.5109579   0.5595181  0.2072289  0.1219277   0.7036145\n",
       " 2        NaN  0.09686747  0.1918072  0.2313253   0.1306024\n",
       " 3        NaN  0.09590361       0.12   0.206747  0.05686747\n",
       " 4        NaN   0.1609639  0.1103614  0.1966265   0.1089157\n",
       " 5        NaN  0.05156627  0.1484337  0.1281928           0\n",
       " 6        NaN  0.03518072  0.2221687  0.1151807           0,\n",
       "    manhattan         tip        dist       speed        pass\n",
       " 1  0.4890421   0.3726083   0.4204431    0.306143   0.6460222\n",
       " 2        NaN   0.1717019   0.2749245   0.4093656    0.193857\n",
       " 3        NaN   0.1671702   0.1409869   0.1621349  0.09566969\n",
       " 4        NaN   0.1696878  0.09415911  0.07200403  0.06445116\n",
       " 5        NaN  0.05740181  0.02517623  0.03776435           0\n",
       " 6        NaN  0.06143001  0.04431017  0.01258812           0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run the classifier over the test sample\n",
    "C=classifyNaiveBayesDiscrete(test,dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7255520504731862"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the classification accuracy over the test set comparing the classification obtained with the labels provided\n",
    "1.0*sum(C==test.manhattan)/len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7049987687761635"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also compute the accuracy over the training set\n",
    "Ct=classifyNaiveBayesDiscrete(train,dp)\n",
    "1.0*sum(Ct==train.manhattan)/len(Ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier (Gaussian)\n",
    "\n",
    "But in many cases the features are arbitrary real-valued (continous) numbers. As before consider a supervised binary classification problem for real-valued vectors $x=(x^i,i=1,...,n)$ given a training set\n",
    "$$\n",
    "\\{(y_j, x_j), j=1..N\\},\n",
    "$$\n",
    "where $x_j=(x^i_j,i=1,...,n)$ are given observed vectors with real-valued components and $y_j$ are the corresponding discrete classifiction labels. \n",
    "\n",
    "Similarly to the discrete framework one can see\n",
    "$$\n",
    "P(y=b|x=x^*)\\sim p(x=x^*|y=b)P(y=b)=P(y=b)\\prod\\limits_{i=1}^n p(x^i={x^*}^i |y=b).\n",
    "$$\n",
    "\n",
    "However in such a case the above framework won't work exactly the same way as we can not directly estimate all the probabilities $P(x^i={x^*}^i|y=b)$. In this case we need to fit a certain continous probability distributions for them, estimating probability densities $p(x^i={x^*}^i|y=b)$ rather than probabilities. In this section we'll present a framework assuming normal (Gaussian) distributions:\n",
    "$$\n",
    "p(x^i={x^*}^i|y=b)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{i,b}}e^{-\\frac{({x^*}^i-\\mu_{i,b})^2}{2\\sigma_{i,b}^2}},\n",
    "$$\n",
    "where parameters $\\mu_{i,b}$ and $\\sigma_{i,b}$ can be simply defined (those willing to have a rigorious probabilistic derivation can try estimating those parameters through max-likelihood approach, which gives the same result) as sample mean and standard deviation:\n",
    "$$\n",
    "\\mu_{i,b}=\\frac{\\sum\\limits_{j:x^i_j={x^*}^i, y_j=b}x^i_j}{|\\{j:x^i_j={x^*}^i, y_j=b\\}|},\n",
    "$$\n",
    "$$\n",
    "\\sigma_{i,b}=\\sqrt{\\frac{\\sum\\limits_{j:x^i_j={x^*}^i, y_j=b}(x^i_j-\\mu_{i,b})^2}{|\\{j:x^i_j={x^*}^i, y_j=b\\}|}}.\n",
    "$$\n",
    "\n",
    "The probabilities $P(y=b)$ can be defined as before:\n",
    "$$\n",
    "P(y=b)=\\frac{|\\{j:y_j=b\\}|}{N}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "P(y=b|x=x^*)\\sim p(x=x^*|y=b)P(y=b)=P(y=b)\\prod\\limits_{i=1}^n p(x^i={x^*}^i |y=b)\\sim\n",
    "$$$$\n",
    "\\sim P(y=b)e^{-\\sum\\limits_{i=1}^n \\frac{({x^*}^i-\\mu_{i,b})^2}{2\\sigma_{i,b}^2}}.\n",
    "$$\n",
    "Finally the clasification level can be defined as\n",
    "$$\n",
    "y^*={\\rm argmax}_{b}\\left[ln(P(y=b))-\\sum\\limits_{i=1}^n\\frac{({x^*}^i-\\mu_{i,b})^2}{2\\sigma_{i,b}^2}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNaiveBayes(trainData):\n",
    "  #training Gausian Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  dp=pd.DataFrame(columns=trainData.columns, index=['mu1','sigma1','mu2','sigma2'])\n",
    "  #estimate priors\n",
    "  dp[trainData.columns[0]]['mu1']=1.0*sum(ind1)/len(trainData.index)\n",
    "  dp[trainData.columns[0]]['mu2']=1.0*sum(ind2)/len(trainData.index)\n",
    "  #estimate sample distribution paramters for p(xi|y=b)\n",
    "  for i in trainData.columns[1:]:\n",
    "    dp.loc['mu1',i]=(trainData[i][ind1]).mean()\n",
    "    dp.loc['sigma1',i]=(trainData[i][ind1]).std()\n",
    "    dp.loc['mu2',i]=(trainData[i][ind2]).mean()\n",
    "    dp.loc['sigma2',i]=(trainData[i][ind2]).std()\n",
    "  return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNaiveBayes(classData,dp):\n",
    "  #classifying using trained Gausian Naive Bayes Classifier\n",
    "  Y=classData.loc[:,classData.columns[0]]*0\n",
    "  for j in classData.index:\n",
    "    #start from the priors\n",
    "    P1=dp[classData.columns[0]]['mu1'];\n",
    "    P2=dp[classData.columns[0]]['mu2'];\n",
    "    #multiply by conditional probability densities p(xi|y=b)\n",
    "    for i in classData.columns[1:]:\n",
    "        if dp[i]['sigma1']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=dp[i]['sigma1'])\n",
    "        \n",
    "        if dp[i]['sigma2']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=dp[i]['sigma2']) \n",
    "    Y[j]=int(P2>P1)\n",
    " \n",
    "\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3. Predicting type of the house\n",
    "\n",
    "Based on the sample of characteristics and the prices of the single unit residential and commercial houses sold in \"our\" zip code 11201 in downtown Brooklyn between the years 2009 and 2012, build a classifier defining if the sold house was actually residential or commercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>area</th>\n",
       "      <th>land</th>\n",
       "      <th>year</th>\n",
       "      <th>price</th>\n",
       "      <th>bldtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2607</td>\n",
       "      <td>1200</td>\n",
       "      <td>2010</td>\n",
       "      <td>825000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1950</td>\n",
       "      <td>1783</td>\n",
       "      <td>1899</td>\n",
       "      <td>1685000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2520</td>\n",
       "      <td>1875</td>\n",
       "      <td>1899</td>\n",
       "      <td>1100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3750</td>\n",
       "      <td>3125</td>\n",
       "      <td>1931</td>\n",
       "      <td>1200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7812</td>\n",
       "      <td>5021</td>\n",
       "      <td>1908</td>\n",
       "      <td>1900000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  area  land  year    price  bldtype\n",
       "0      1  2607  1200  2010   825000        0\n",
       "1      2  1950  1783  1899  1685000        0\n",
       "2      3  2520  1875  1899  1100000        0\n",
       "3      4  3750  3125  1931  1200000        1\n",
       "4      5  7812  5021  1908  1900000        1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the sample\n",
    "data = pd.read_csv(\"data/NYC_RE_sample.csv\")\n",
    "# split sample into training and test ones\n",
    "data.columns=['index','area','land','year','price','bldtype'] #house size, land, year built, price and type\n",
    "data.bldtype=data.bldtype-1 #bring the house classes to 0 - residential, 1 - commercial\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform the features to relative values getting rid of obvious correlations: consider price per sq. foot, land per sq. foot of indoor area\n",
    "data1=pd.DataFrame({'Y':data.bldtype,'price':data.price/data.area,'land':data.land/data.area,'year':data.year})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>land</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.914359</td>\n",
       "      <td>864.102564</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.666966</td>\n",
       "      <td>1049.160671</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>510.204082</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Y      land        price  year\n",
       "1   0  0.914359   864.102564  1899\n",
       "3   1  0.833333   320.000000  1931\n",
       "6   0  0.666966  1049.160671  1901\n",
       "8   1  0.200000   340.000000  1900\n",
       "10  0  0.510204   510.204082  1901"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset into 60% training and 40% test \n",
    "np.random.seed(2015)\n",
    "ind=stats.bernoulli.rvs(p = 0.6, size = len(data.index))\n",
    "train=data1[ind==1]\n",
    "test=data1[ind==0]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp=trainNaiveBayes(train) #train the Gausian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>land</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu1</th>\n",
       "      <td>0.6792453</td>\n",
       "      <td>0.6391921</td>\n",
       "      <td>1010.221</td>\n",
       "      <td>1907.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2639759</td>\n",
       "      <td>388.859</td>\n",
       "      <td>46.07333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu2</th>\n",
       "      <td>0.3207547</td>\n",
       "      <td>0.9142577</td>\n",
       "      <td>384.538</td>\n",
       "      <td>1928.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9183005</td>\n",
       "      <td>264.8162</td>\n",
       "      <td>21.81203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Y       land     price      year\n",
       "mu1     0.6792453  0.6391921  1010.221  1907.139\n",
       "sigma1        NaN  0.2639759   388.859  46.07333\n",
       "mu2     0.3207547  0.9142577   384.538  1928.529\n",
       "sigma2        NaN  0.9183005  264.8162  21.81203"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C=classifyNaiveBayes(test,dp) #classify test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.92682926829268"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classification accuracy (over test set)\n",
    "100.0*sum(C==test.Y)/len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.67924528301887"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also report accuracy of the classifier over the training - it is slightly higher, although not that higher\n",
    "Ct=classifyNaiveBayes(train,dp)\n",
    "100.0*sum(Ct==train.Y)/len(Ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4. Spam classification\n",
    "\n",
    "1. Title:  SPAM E-mail Database\n",
    "\n",
    "2. Sources:\n",
    "   (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
    "        Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
    "   (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
    "   (c) Generated: June-July 1999\n",
    "\n",
    "3. Past Usage:\n",
    "   (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n",
    "   (b) Determine whether a given email is spam or not.\n",
    "   (c) ~7% misclassification error.\n",
    "       False positives (marking good mail as spam) are very undesirable.\n",
    "       If we insist on zero false positives in the training/testing set,\n",
    "       20-25% of the spam passed through the filter.\n",
    "\n",
    "4. Relevant Information:\n",
    "        The \"spam\" concept is diverse: advertisements for products/web\n",
    "        sites, make money fast schemes, chain letters, pornography...\n",
    "\tThe collection of spam e-mails came from the postmaster and \n",
    "\tindividuals who had filed spam.  The collection of non-spam \n",
    "\te-mails came from filed work and personal e-mails, and hence\n",
    "\tthe word 'george' and the area code '650' are indicators of \n",
    "\tnon-spam.  These are useful when constructing a personalized \n",
    "\tspam filter.  One would either have to blind such non-spam \n",
    "\tindicators or get a very wide collection of non-spam to \n",
    "\tgenerate a general purpose spam filter.\n",
    "\n",
    "        For background on spam:\n",
    "        Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n",
    "        Communications of the ACM, 41(8):74-83, 1998.\n",
    "\n",
    "5. Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "\n",
    "6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    "7. Attribute Information:\n",
    "The last column of 'spambase.data' denotes whether the e-mail was \n",
    "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
    "Most of the attributes indicate whether a particular word or\n",
    "character was frequently occuring in the e-mail.  The run-length\n",
    "attributes (55-57) measure the length of sequences of consecutive \n",
    "capital letters.  For the statistical measures of each attribute, \n",
    "see the end of this file.  Here are the definitions of the attributes:\n",
    "\n",
    "48 continuous real [0,100] attributes of type word_freq_WORD \n",
    "= percentage of words in the e-mail that match WORD,\n",
    "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
    "total number of words in e-mail.  A \"word\" in this case is any \n",
    "string of alphanumeric characters bounded by non-alphanumeric \n",
    "characters or end-of-string.\n",
    "\n",
    "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
    "= percentage of characters in the e-mail that match CHAR,\n",
    "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "1 continuous real [1,...] attribute of type capital_run_length_average\n",
    "= average length of uninterrupted sequences of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
    "= length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
    "= sum of length of uninterrupted sequences of capital letters\n",
    "= total number of capital letters in the e-mail\n",
    "\n",
    "1 nominal {0,1} class attribute of type spam\n",
    "= denotes whether the e-mail was considered spam (1) or not (0), \n",
    "i.e. unsolicited commercial e-mail.  \n",
    "\n",
    "\n",
    "8. Missing Attribute Values: None\n",
    "\n",
    "9. Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "data = urllib.urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\").read()\n",
    "data_name=urllib.urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\").read()\n",
    "# Read the data\n",
    "data=data.split(\"\\r\\n\")\n",
    "data_spam=[]\n",
    "for i in range(len(data)):\n",
    "    if len(data[i])>0:\n",
    "        temp=data[i].split(\",\")\n",
    "        #change from str to float\n",
    "        t_l=[]\n",
    "        for j in range(len(temp)):\n",
    "            t_l.append(float(temp[j]))\n",
    "        data_spam.append(t_l)\n",
    "\n",
    "#Read the column names\n",
    "temp=data_name.split(\"\\r\\n\")\n",
    "column_names=[]\n",
    "for i in temp:\n",
    "    if (i.startswith('word') or i.startswith('char') or i.startswith('capital')):\n",
    "        column_names.append(i.split(\":\")[0])\n",
    "column_names.append(\"spam\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam  word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0     1            0.00               0.64           0.64             0   \n",
       "1     1            0.21               0.28           0.50             0   \n",
       "2     1            0.06               0.00           0.71             0   \n",
       "3     1            0.00               0.00           0.00             0   \n",
       "4     1            0.00               0.00           0.00             0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order            ...             word_freq_conference  \\\n",
       "0             0.00            ...                                0   \n",
       "1             0.00            ...                                0   \n",
       "2             0.64            ...                                0   \n",
       "3             0.31            ...                                0   \n",
       "4             0.31            ...                                0   \n",
       "\n",
       "   char_freq_;  char_freq_(  char_freq_[  char_freq_!  char_freq_$  \\\n",
       "0         0.00        0.000            0        0.778        0.000   \n",
       "1         0.00        0.132            0        0.372        0.180   \n",
       "2         0.01        0.143            0        0.276        0.184   \n",
       "3         0.00        0.137            0        0.137        0.000   \n",
       "4         0.00        0.135            0        0.135        0.000   \n",
       "\n",
       "   char_freq_#  capital_run_length_average  capital_run_length_longest  \\\n",
       "0        0.000                       3.756                          61   \n",
       "1        0.048                       5.114                         101   \n",
       "2        0.010                       9.821                         485   \n",
       "3        0.000                       3.537                          40   \n",
       "4        0.000                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                       278  \n",
       "1                      1028  \n",
       "2                      2259  \n",
       "3                       191  \n",
       "4                       191  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate the data set for our tests\n",
    "data_spam=pd.DataFrame(data_spam)\n",
    "data_spam.columns=column_names\n",
    "X=data_spam.iloc[:,0:-1]\n",
    "y=data_spam.iloc[:,-1]\n",
    "data_spam=pd.concat([y,X], axis=1)\n",
    "data_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the data into test and traing sets\n",
    "random.seed(2015)\n",
    "train_index=random.sample(list(range(len(X))),int(len(X)*0.8))\n",
    "test_index=[x for x in list(range(len(X))) if x not in train_index]\n",
    "X_train=X.iloc[train_index,:]\n",
    "X_test=X.iloc[test_index,:]\n",
    "y_train=y.iloc[train_index]\n",
    "y_test=y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu1</th>\n",
       "      <td>0.6089674</td>\n",
       "      <td>0.06809014</td>\n",
       "      <td>0.2271709</td>\n",
       "      <td>0.2000892</td>\n",
       "      <td>0.0009638554</td>\n",
       "      <td>0.1855734</td>\n",
       "      <td>0.04554217</td>\n",
       "      <td>0.007920571</td>\n",
       "      <td>0.03915663</td>\n",
       "      <td>0.03802767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04741633</td>\n",
       "      <td>0.05046006</td>\n",
       "      <td>0.1602102</td>\n",
       "      <td>0.020888</td>\n",
       "      <td>0.1164801</td>\n",
       "      <td>0.01184293</td>\n",
       "      <td>0.02108121</td>\n",
       "      <td>2.3761</td>\n",
       "      <td>18.32664</td>\n",
       "      <td>159.0553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2657866</td>\n",
       "      <td>1.547869</td>\n",
       "      <td>0.5011758</td>\n",
       "      <td>0.02287893</td>\n",
       "      <td>0.6263632</td>\n",
       "      <td>0.2278229</td>\n",
       "      <td>0.08464661</td>\n",
       "      <td>0.2492644</td>\n",
       "      <td>0.2054897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3397216</td>\n",
       "      <td>0.3054744</td>\n",
       "      <td>0.2468942</td>\n",
       "      <td>0.1126406</td>\n",
       "      <td>0.9015804</td>\n",
       "      <td>0.07218688</td>\n",
       "      <td>0.2269393</td>\n",
       "      <td>5.533877</td>\n",
       "      <td>41.44568</td>\n",
       "      <td>337.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu2</th>\n",
       "      <td>0.3910326</td>\n",
       "      <td>0.1588047</td>\n",
       "      <td>0.16836</td>\n",
       "      <td>0.3985615</td>\n",
       "      <td>0.1753231</td>\n",
       "      <td>0.5122307</td>\n",
       "      <td>0.1727728</td>\n",
       "      <td>0.2825295</td>\n",
       "      <td>0.2103127</td>\n",
       "      <td>0.1691522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002182071</td>\n",
       "      <td>0.02077137</td>\n",
       "      <td>0.1091543</td>\n",
       "      <td>0.007288395</td>\n",
       "      <td>0.5123878</td>\n",
       "      <td>0.1685726</td>\n",
       "      <td>0.08813899</td>\n",
       "      <td>9.962755</td>\n",
       "      <td>107.4295</td>\n",
       "      <td>475.5949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3236926</td>\n",
       "      <td>0.3616037</td>\n",
       "      <td>0.4681438</td>\n",
       "      <td>2.400083</td>\n",
       "      <td>0.7161839</td>\n",
       "      <td>0.3137195</td>\n",
       "      <td>0.605078</td>\n",
       "      <td>0.5659269</td>\n",
       "      <td>0.3532787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02726459</td>\n",
       "      <td>0.09269251</td>\n",
       "      <td>0.3088427</td>\n",
       "      <td>0.04689625</td>\n",
       "      <td>0.7348309</td>\n",
       "      <td>0.3257907</td>\n",
       "      <td>0.6805679</td>\n",
       "      <td>53.7536</td>\n",
       "      <td>324.751</td>\n",
       "      <td>867.4377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             spam word_freq_make word_freq_address word_freq_all  \\\n",
       "mu1     0.6089674     0.06809014         0.2271709     0.2000892   \n",
       "sigma1        NaN      0.2657866          1.547869     0.5011758   \n",
       "mu2     0.3910326      0.1588047           0.16836     0.3985615   \n",
       "sigma2        NaN      0.3236926         0.3616037     0.4681438   \n",
       "\n",
       "        word_freq_3d word_freq_our word_freq_over word_freq_remove  \\\n",
       "mu1     0.0009638554     0.1855734     0.04554217      0.007920571   \n",
       "sigma1    0.02287893     0.6263632      0.2278229       0.08464661   \n",
       "mu2        0.1753231     0.5122307      0.1727728        0.2825295   \n",
       "sigma2      2.400083     0.7161839      0.3137195         0.605078   \n",
       "\n",
       "       word_freq_internet word_freq_order           ...             \\\n",
       "mu1            0.03915663      0.03802767           ...              \n",
       "sigma1          0.2492644       0.2054897           ...              \n",
       "mu2             0.2103127       0.1691522           ...              \n",
       "sigma2          0.5659269       0.3532787           ...              \n",
       "\n",
       "       word_freq_conference char_freq_; char_freq_(  char_freq_[ char_freq_!  \\\n",
       "mu1              0.04741633  0.05046006   0.1602102     0.020888   0.1164801   \n",
       "sigma1            0.3397216   0.3054744   0.2468942    0.1126406   0.9015804   \n",
       "mu2             0.002182071  0.02077137   0.1091543  0.007288395   0.5123878   \n",
       "sigma2           0.02726459  0.09269251   0.3088427   0.04689625   0.7348309   \n",
       "\n",
       "       char_freq_$ char_freq_# capital_run_length_average  \\\n",
       "mu1     0.01184293  0.02108121                     2.3761   \n",
       "sigma1  0.07218688   0.2269393                   5.533877   \n",
       "mu2      0.1685726  0.08813899                   9.962755   \n",
       "sigma2   0.3257907   0.6805679                    53.7536   \n",
       "\n",
       "       capital_run_length_longest capital_run_length_total  \n",
       "mu1                      18.32664                 159.0553  \n",
       "sigma1                   41.44568                  337.002  \n",
       "mu2                      107.4295                 475.5949  \n",
       "sigma2                    324.751                 867.4377  \n",
       "\n",
       "[4 rows x 58 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata=pd.concat([y_train,X_train],axis=1)\n",
    "dp=trainNaiveBayes(traindata)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OS Test\n",
    "testdata=pd.concat([y_test,X_test],axis=1)\n",
    "C=classifyNaiveBayes(testdata,dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we classified e-mails correctly in 80.6731813246 percents of cases\n"
     ]
    }
   ],
   "source": [
    "print \"we classified e-mails correctly in {0} percents of cases\".format(100.0*sum(C==y_test)/len(y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But among 20% of errors, 16.2866449511 percent are ham classified as spam\n"
     ]
    }
   ],
   "source": [
    "print \"But among 20% of errors, {0} percent are ham classified as spam\".format(100.0*sum((C==1)&(y_test==0))/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a result 92.513368984 percent of spam e-mails are detected\n"
     ]
    }
   ],
   "source": [
    "print \"As a result {0} percent of spam e-mails are detected\".format(100.0*sum((C==1)&(y_test==1))/sum(y_test==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But 27.4223034735 percent of non-spam e-mails got classified as spam\n"
     ]
    }
   ],
   "source": [
    "print \"But {0} percent of non-spam e-mails got classified as spam\".format(100.0*sum((C==1)&(y_test==0))/sum(y_test==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the price of two different types of error - a) false negatives, i.e. missed spam or b) false positives, i.e. ham classified as spam - is quite different. In the first case we only get certain inconvenience of still getting some spam e-mails missed by our classifier, but in the second case we can actually miss important information from ham e-mails classified as spam.\n",
    "\n",
    "So in this specific case the false positives should be additionally penalized. Or we can set a threshold for our level of tolerance to false positives - say we allow no more than 1% of ham be classified as spam. The way to adjust our classifier is to let it classify the e-mail as \"spam\" not just whenever it seems more likely to be spam than ham, i.e. when $P(spam|x)> P(ham|x)$, but only when we're strongly confident, i.e. when \n",
    "$$\n",
    "P(spam|x)>\\lambda P(ham|x),\n",
    "$$\n",
    "where $\\lambda$ is a selected constant, higher than $1$. The higher is $\\lambda$ the stronger is the criteria for spam classification and the lower is the tolerance to false positives. As those posterior probabilities $P(spam|x)$ and $P(ham|x)$ are proportional to the priors $P(spam)$, $P(ham)$, the other equivalent way to implement such an adjustment rather than by introducing a constant $\\lambda$ on the decision step, is through modifying the prior as $P(spam|x):=P(spam|x)/\\alpha$, where $\\alpha>1$. This means that we deliberately lower our prior belief that the e-mail is spam making it harder for the classifier to alter this opinion.\n",
    "\n",
    "In practice, selection of the constant $\\lambda$ or $\\alpha$ could be performed in an empiric way once validation sample is separated from the training one - we keep increasing it until the rate of false positives over the validation sample becomes satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the training data into new training and validation sets\n",
    "random.seed(2015)\n",
    "train_index1=random.sample(list(range(len(y_train))),int(len(y_train)*0.7))\n",
    "valid_index=[x for x in list(range(len(y_train))) if x not in train_index1]\n",
    "traindata1=traindata.loc[train_index1]\n",
    "validdata=traindata.loc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially we classified e-mails correctly in 82.4104234528 percents of cases with 24.1316270567 percent of ham classified as spam\n",
      "False positives = 13.6771300448 percent\n",
      "False positives = 6.05381165919 percent\n",
      "False positives = 4.03587443946 percent\n",
      "False positives = 3.13901345291 percent\n",
      "False positives = 2.69058295964 percent\n",
      "We classified e-mails correctly in 85.016286645 percents of cases with only 4.38756855576 percent of ham classified as spam\n"
     ]
    }
   ],
   "source": [
    "dpl=trainNaiveBayes(traindata1)\n",
    "C=classifyNaiveBayes(testdata,dpl)\n",
    "fp=100.0*sum((C==1)&(testdata.spam==0))/sum(testdata.spam==0)\n",
    "acc=format(100.0*sum(C==testdata.spam)/len(testdata.spam))\n",
    "print \"Initially we classified e-mails correctly in {0} percents of cases with {1} percent of ham classified as spam\".format(acc,fp)     \n",
    "while True:\n",
    "    dpl.spam[2]=dpl.spam[2]/1000 #lower the prior P(spam) - need to low it very quickly as due to the high amount of regressors only really low prior would have a major impact on the classifier\n",
    "    dpl.spam[0]=1-dpl.spam[2]\n",
    "    C=classifyNaiveBayes(validdata,dpl)\n",
    "    fp=100.0*sum((C==1)&(validdata.spam==0))/sum(validdata.spam==0)\n",
    "    print \"False positives = {0} percent\".format(fp)\n",
    "    if fp<3: #once the percentage of false positives goes below 3%\n",
    "         break\n",
    "C=classifyNaiveBayes(testdata,dpl)\n",
    "fp=100.0*sum((C==1)&(testdata.spam==0))/sum(testdata.spam==0)\n",
    "acc=format(100.0*sum(C==testdata.spam)/len(testdata.spam))\n",
    "print \"We classified e-mails correctly in {0} percents of cases with only {1} percent of ham classified as spam\".format(acc,fp)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised EM classifier\n",
    "\n",
    "This is typically done based on the Expectation Maximization (EM) algorithm. As before assume we have a set of observations for the input variable vectors $x=(x^i,i=1,...,n)$ as well as the corresponding labels (discrete output variable $y$)\n",
    "$$\n",
    "\\{(y_j, x_j), j=1..N\\},\n",
    "$$\n",
    "where $x_j=(x^i_j,i=1,...,n)$ are given observed vectors of discrete features and $y_j$ are the corresponding classifiction labels. However suppose that some of the $y_j$ are not known and let $y_j=nan$ in this case.\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "Step 1. Based on the labeled subset of the training data (i.e. having $y_j\\neq nan$) estimate sample conditional probabilities $\\theta_{b,i,a}^0=P(x^i=a|y=b)$ as well as sample prior probabilities $\\theta_b^0=p(y=b)$ for each of the possible values $b$ of $y$ and $a$ of $x^i$. Set $t=0$.\n",
    "\n",
    "Step 2. Given the current estimate $\\theta=\\theta^t$ for $P(x^i|y=b)$ and $P(y=b)$ define the unobserved labels $y_j$ as the discrete random variables $\\hat{y}_j$ with the probability distribution:\n",
    "$$\n",
    "P(\\hat{y}_j=b|x_j,\\theta^t)=\\frac{P(y=b)\\prod\\limits_{i=1}^n P(x^i_j |y=b)}{\\sum_c P(y=c)\\prod\\limits_{i=1}^n P(x^i_j |y=c)}=\\frac{\\theta_b^t\\prod\\limits_{i=1}^n \\theta_{b,i,x^i_j}^t}{\\sum_c \\theta_c^t\\prod\\limits_{i=1}^n \\theta_{c,i,x^i_j}^t}.\n",
    "$$\n",
    "In case the label $y_j$ is observed we simply let\n",
    "$$\n",
    "P(\\hat{y}_j=b|x_j,\\theta^t)=\\left\\{\\begin{array}{c}1, b=y_j,\\\\0, b\\neq y_j.\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Step 3. Re-estimate the parameters $\\theta=\\theta^{t+1}$ (maximizing the likelihood of the actual observations, see below) for the distributions of $P(x^i|y=b)$ as well as $P(y=b)$ given the label probabilistic estimates with respect to the probabilities $P(\\hat{y}_j=b|x_j,\\theta^t)$ defined in step 2.\n",
    "\n",
    "Step 4. If $\\theta$ does not change much, i.e. if the termination condition\n",
    "$$\n",
    "||\\theta^{t+1}-\\theta^t||_2=\\sum_b (\\theta_b^{t+1}-\\theta_b^t)^2+\\sum_{k,i,a} (\\theta_{k,i,a}^{t+1}-\\theta_{k,i,a}^t)^2<\\varepsilon\n",
    "$$ \n",
    "holds - stop with a final estimate $\\theta=\\theta^{t+1}$, otherwise set $t:=t+1$ and repeat from step 2.\n",
    "\n",
    "An important component of the algorithm to be clarified is estimating the parameters $\\theta^{t+1}$. It is based on a max-likelihood approach, which can be shown to be a streightforward generalization of the sample probability estimates with respect to the label estimate uncertainty. Specifically one could let:\n",
    "$$\n",
    "\\theta_b^{t+1}=P(y=b)=\\frac{\\sum\\limits_j P(\\hat{y}_j=b|x^j,\\theta^t)}{|\\{j\\}|}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\theta_{b,i,a}^{t+1}=P(x_i=a|y=b)=\\frac{\\sum\\limits_{j,x_i^j=a} P(\\hat{y}_j=b|x^j,\\theta^t)}{\\sum\\limits_j P(\\hat{y}_j=b|x^j,\\theta^t)}.\n",
    "$$\n",
    "\n",
    "At each step those choices of $\\theta$ are intended to maximize the log-likelihood of observing all the $x^j_i$ we are given, which can be defined as\n",
    "$$\n",
    "L(\\theta)=\\sum\\limits_j log\\left(\\sum\\limits_b P(y=b)\\prod\\limits_i P(x_i=x^j_i|y=b)\\right)=\\sum\\limits_j log\\left(\\sum\\limits_b \\theta_b\\prod\\limits_i \\theta_{b,i,x^i_j}\\right).\n",
    "$$\n",
    "\n",
    "Finally, once the iterative EM algorithm helps to infer $\\theta$, the classification of new any observations $x^*=\\{{x^*}^i\\}$ is pretty streightforward and goes pretty much along the lines of the Naive Bayes classifier:\n",
    "$$\n",
    "y^*={\\rm argmax}_{b}P(y=b)\\prod\\limits_{i=1}^n P(x^i={x^*}^i |y=b)={\\rm argmax}_{b}\\theta_b\\prod\\limits_{i=1}^n \\theta_{b,i,{x^i}^*}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5. Taxi trip classification with partially missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manhattan</th>\n",
       "      <th>tip</th>\n",
       "      <th>dist</th>\n",
       "      <th>speed</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26066</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26100</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26354</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26589</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26722</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       manhattan  tip  dist  speed  pass\n",
       "26066          1    1     1      1     1\n",
       "26100          1    3     5      6     1\n",
       "26354          1    1     6      4     2\n",
       "26589          0    2     2      2     1\n",
       "26722          1    4     1      1     1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the same data for the taxi trip as used before\n",
    "data = pd.read_csv(\"data/NYC_taxi_sample.csv\",index_col=0)\n",
    "data.manhattan=2-data.manhattan\n",
    "#data structure: categorical variables showing if the trip was within Mangattan and how big/small were the distance travelled, speed, tip left as well as how many passengers were on the trip\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manhattan</th>\n",
       "      <th>tip</th>\n",
       "      <th>dist</th>\n",
       "      <th>speed</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26066</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26100</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26354</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26722</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26793</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       manhattan  tip  dist  speed  pass\n",
       "26066          1    1     1      1     1\n",
       "26100          1    3     5      6     1\n",
       "26354          1    1     6      4     2\n",
       "26722          1    4     1      1     1\n",
       "26793          0    3     3      3     2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as before take approximately 75% of the dataset for the training set, reserving the rest for the test set\n",
    "np.random.seed(2014)\n",
    "ind=stats.bernoulli.rvs(p = 0.75, size = len(data.index))\n",
    "train=data[ind==1]\n",
    "test=data[ind==0]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split output variable from the features\n",
    "X=train.iloc[:,1:]\n",
    "y=train.iloc[:,0]\n",
    "X_test=test.iloc[:,1:]\n",
    "y_test=test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now let's randomly delete 99.5% of the labels from the training set\n",
    "random.seed(2015)\n",
    "Label_index=random.sample(list(range(len(X))),int(len(X)*0.005))\n",
    "Unlabel_index=[x for x in list(range(len(X))) if x not in Label_index]\n",
    "\n",
    "X_Label=X.iloc[Label_index,:]\n",
    "X_Unlabel=X.iloc[Unlabel_index,:]   \n",
    "y_Label=y.iloc[Label_index]\n",
    "y_Unlabel=y.iloc[Unlabel_index]# we should not use this information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we rely only on the labeled data training Naive Bayes Classifier over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData=pd.concat([y_Label,X_Label],axis=1)\n",
    "len(y_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   manhattan         tip        dist       speed        pass\n",
       " 1  0.7368421         0.5   0.1428571   0.1428571   0.5714286\n",
       " 2        NaN  0.07142857   0.3571429   0.4285714   0.2142857\n",
       " 3        NaN   0.2142857   0.2142857  0.07142857   0.1428571\n",
       " 4        NaN  0.07142857   0.2142857   0.2857143  0.07142857\n",
       " 5        NaN           0           0           0           0\n",
       " 6        NaN   0.1428571  0.07142857  0.07142857           0,\n",
       "    manhattan  tip dist speed pass\n",
       " 1  0.2631579  0.6  0.2   0.2  0.8\n",
       " 2        NaN    0    0   0.2    0\n",
       " 3        NaN    0  0.2   0.4    0\n",
       " 4        NaN  0.2  0.2     0  0.2\n",
       " 5        NaN    0  0.2     0    0\n",
       " 6        NaN  0.2  0.2   0.2    0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp=trainNaiveBayesDiscrete(trainData)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata=pd.concat([y_test,X_test],axis=1)\n",
    "C=classifyNaiveBayesDiscrete(testdata,dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We correctly classified 50.8148148148 percents of the trips based on the labeled data only\n"
     ]
    }
   ],
   "source": [
    "acc=format(100.0*sum(C==y_test)/len(y_test))\n",
    "print \"We correctly classified {0} percents of the trips based on the labeled data only\".format(acc)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#implementation of Expectation-Maximization algorithm for partially labeled data\n",
    "def EM(X_Label,y_Label,X_Unlabel,dp):\n",
    "  t = 0  \n",
    "  haslabels=len(y_Label)>0\n",
    "\n",
    "  while True:\n",
    "    t = t + 1\n",
    "\n",
    "    classData=X_Unlabel\n",
    "    # Now we want to calculate P(y=1|x) and P(y=2|x) for all observations xj. (these are bunch of scalars)\n",
    "    # we need this to calculate new dp. Basically speaking, for every new iteration we need a new dp.\n",
    "\n",
    "    #for y=1 and y=2\n",
    "\n",
    "    p_x_1=[] #unnormalized P(y=1|x)\n",
    "    p_x_2=[] #unnormalized P(y=2|x)\n",
    "    cols=dp[0].columns\n",
    "\n",
    "    for i in classData.index:\n",
    "        P1=dp[0][cols[0]][1];\n",
    "        P2=dp[1][cols[0]][1];\n",
    "        for j in classData.columns:\n",
    "            P1=P1*dp[0][j][classData[j][i]]\n",
    "            P2=P2*dp[1][j][classData[j][i]]\n",
    "        p_x_1.append(P1)\n",
    "        p_x_2.append(P2)\n",
    "\n",
    "    #Rescale p_x_1 and p_x_2:\n",
    "    summ=np.asarray(p_x_1)+np.asarray(p_x_2)\n",
    "    p_x_1_s=np.asarray(p_x_1)/summ\n",
    "    p_x_2_s=np.asarray(p_x_2)/summ\n",
    "    inds_1 = np.where(np.isnan(p_x_1_s))\n",
    "    inds_2 = np.where(np.isnan(p_x_2_s))\n",
    "    p_x_1_s[inds_1]=0.5\n",
    "    p_x_2_s[inds_2]=0.5\n",
    "    #Now let's calculate P(y=1) and P(y=2)\n",
    "    p_1=p_x_1_s.sum()/len(p_x_1_s)\n",
    "    p_2=p_x_2_s.sum()/len(p_x_2_s)\n",
    "\n",
    "\n",
    "    #Now let's calculate the probability distribution of P(xi|y=1) and P(xi|y=2)\n",
    "    \n",
    "    m=max([classData[j][i] for j in classData.columns for i in classData.index]) #maximal number of classes in each feature of a training set\n",
    "\n",
    "    #create output data structure for the probabilities - new iteration\n",
    "    \n",
    "    dp1=[pd.DataFrame(columns=cols, index=range(1,m+1)), pd.DataFrame(columns=cols, index=range(1,m+1))]\n",
    "\n",
    "    #P(y=b)  \n",
    "    dp1[0][cols[0]][1]=p_1\n",
    "    dp1[1][cols[0]][1]=p_2\n",
    "\n",
    "\n",
    "    #estimate conditional probabilities P(x|y=b) -do we add labeled data to fit?\n",
    "\n",
    "    temp=np.concatenate((np.asmatrix(X_Unlabel),np.asarray(pd.DataFrame(p_x_1_s)),np.asarray(pd.DataFrame(p_x_2_s))), axis=1)\n",
    "    temp=pd.DataFrame(temp)\n",
    "    if haslabels:\n",
    "        temp_l=np.concatenate((np.asmatrix(X_Label),np.asmatrix(1*(y_Label==0)).transpose(),np.asmatrix(1*(y_Label==1)).transpose()),axis=1)\n",
    "        temp_l=pd.DataFrame(temp_l)\n",
    "        pd.concat([temp,temp_l])\n",
    "   \n",
    "\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        for i in range(len(dp[0])):\n",
    "\n",
    "            dp1[0].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-2].sum()/temp.iloc[:,-2].sum()\n",
    "            dp1[1].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-1].sum()/temp.iloc[:,-1].sum()\n",
    "\n",
    "        ############################################################################################\n",
    "    # Now we use dp to decide whether to continue our iterations\n",
    "    \n",
    "    if (sum(sum((dp1[0]-dp[0])**2))+sum(sum((dp1[1]-dp[1])**2)))<0.001: #if dp does not change much\n",
    "        break\n",
    "    else: \n",
    "        dp=dp1  #save new dp and perform next iteration\n",
    "\n",
    "        \n",
    "    ###############################################################################################\n",
    "        #Calculate the log-likelihood\n",
    "        \n",
    "        L=0\n",
    "        \n",
    "        for i in classData.index:\n",
    "            P1=dp[0][cols[0]][1];\n",
    "            P2=dp[1][cols[0]][1];\n",
    "            for j in classData.columns:\n",
    "                P1=P1*dp[0][j][classData[j][i]]\n",
    "                P2=P2*dp[1][j][classData[j][i]]\n",
    "            temp=math.log(P1+P2)\n",
    "            L=L+temp\n",
    "        if haslabels:    \n",
    "          for i in X_Label.index:\n",
    "            yi=y_Label[i]\n",
    "            P=dp[yi][cols[0]][1];\n",
    "            for j in X_Label.columns:\n",
    "                P=P*dp[yi][j][X_Label[j][i]]\n",
    "            L=L+math.log(P)\n",
    "        \n",
    "        print \"Iteration {0}: log maximum liklihood = {1}\".format(t,L)    \n",
    "        \n",
    "        \n",
    "  return dp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -23104.910287\n",
      "Iteration 2: log maximum liklihood = -22956.539844\n",
      "Iteration 3: log maximum liklihood = -22733.3814326\n",
      "Iteration 4: log maximum liklihood = -22418.4482997\n",
      "Iteration 5: log maximum liklihood = -22144.6817837\n",
      "Iteration 6: log maximum liklihood = -22030.6757311\n",
      "Iteration 7: log maximum liklihood = -21997.1619209\n",
      "Iteration 8: log maximum liklihood = -21981.4247846\n",
      "After EM we correctly classified 67.2592592593 percents of the trips\n"
     ]
    }
   ],
   "source": [
    "#perform EM estimation for theta\n",
    "dpEM=EM(X_Label,y_Label,X_Unlabel,dp)\n",
    "#OS test\n",
    "C=classifyNaiveBayesDiscrete(testdata,dpEM) #classify test data with a new theta given by EM\n",
    "acc=100.0*sum(C==y_test)/len(y_test)\n",
    "print \"After EM we correctly classified {0} percents of the trips\".format(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised EM clustering\n",
    "\n",
    "As one can see from the semi-supervised EM framework, the observed portion of the labels have a relatively low impact on the final estimate - they only participate in the initialization of $\\theta^0$ and further override the uncertainty for the estimates $\\hat{y}_j$ (while those uncertain estimates can actually be available for the observed labels in the same way as for the unobserved ones, already without taking into account the original values $y_j$).\n",
    "\n",
    "So what if none of the labels $y_j$ are observed? What it changes in the above framework is only the initialization step - as now we do not have any reasonable guess for $\\theta^0$ we simply start from a random or homogeneous choice of those probabilities, provided that the probabilities are notmalized appropriately, i.e. $\\sum_k \\theta_k^0=1$ and $\\sum_a \\theta_{k,i,a}^0=1$. Once $\\theta^0$ is selected the steps 2-4 of the algorithm are exactly the same as before (just having all the $y_j=nan$, i.e. no estimates $\\hat{y}_j$ to fix).\n",
    "\n",
    "This way EM algorithm then enables clustering for the observations $x_j$, inferring the underlying probabilities $\\theta$, unobserved labels $y_j$ and training a classifier for any future observations $x^*$ as before:\n",
    "$$\n",
    "y^*={\\rm argmax}_{b}P(y=b)\\prod\\limits_{i=1}^n P(x^i={x^*}^i |y=b)={\\rm argmax}_{b}\\theta_b\\prod\\limits_{i=1}^n \\theta_{b,i,{x^i}^*}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example 6. Taxi trip classification with no labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_Unlabel=X #take all observations unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2016)\n",
    "#initialize theta randomly\n",
    "dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[0]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[0].iloc[:,j]=b\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[1]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[1].iloc[:,j]=b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   manhattan       tip      dist     speed      pass\n",
       " 1  0.8967054  0.203711  0.233433  0.052900  0.275895\n",
       " 2        NaN  0.218506  0.246143  0.221832  0.275194\n",
       " 3        NaN  0.206894  0.205944  0.216376  0.020212\n",
       " 4        NaN  0.128907  0.191068  0.025485  0.103416\n",
       " 5        NaN  0.179253  0.079939  0.346481  0.109967\n",
       " 6        NaN  0.062729  0.043473  0.136927  0.215316,\n",
       "    manhattan       tip      dist     speed      pass\n",
       " 1  0.1032946  0.145768  0.248003  0.008330  0.138931\n",
       " 2        NaN  0.118057  0.187649  0.327176  0.128262\n",
       " 3        NaN  0.159788  0.189117  0.167362  0.298924\n",
       " 4        NaN  0.346520  0.101663  0.130922  0.113730\n",
       " 5        NaN  0.161480  0.147634  0.289818  0.229165\n",
       " 6        NaN  0.068387  0.125933  0.076393  0.090988]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -23151.4496838\n",
      "Iteration 2: log maximum liklihood = -23102.1686162\n",
      "Iteration 3: log maximum liklihood = -23066.4449969\n",
      "Iteration 4: log maximum liklihood = -23038.4978789\n",
      "Iteration 5: log maximum liklihood = -23007.8977578\n",
      "Iteration 6: log maximum liklihood = -22947.8175748\n",
      "Iteration 7: log maximum liklihood = -22799.4324681\n",
      "Iteration 8: log maximum liklihood = -22511.6032021\n",
      "Iteration 9: log maximum liklihood = -22203.7134995\n",
      "Iteration 10: log maximum liklihood = -22039.6582528\n",
      "Iteration 11: log maximum liklihood = -21981.8045475\n",
      "Iteration 12: log maximum liklihood = -21957.5491092\n",
      "Iteration 13: log maximum liklihood = -21943.1487191\n",
      "After EM we correctly classified 67.7037037037 percents of the trips\n"
     ]
    }
   ],
   "source": [
    "#perform EM estimation for theta\n",
    "dpEM=EM([],[],X_Unlabel,dp)\n",
    "#OS test\n",
    "C=classifyNaiveBayesDiscrete(testdata,dpEM) #classify test data with a new theta given by EM\n",
    "acc=100.0*sum(C==y_test)/len(y_test)\n",
    "if acc<50: #if most of the labels are misclassified it is just the matter of swapping the labels 1<->0\n",
    "    acc=100-acc\n",
    "print \"After EM we correctly classified {0} percents of the trips\".format(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
